{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D7hJlilKM485"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gomesgr2/PLN-Projects/blob/main/PLN_ATIVIDADE_PR%C3%81TICA_02_ipynb_Grupo_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 02 [Extração e Pré-processamento de Dados + Expressões Regulares]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 02** deve ser feita utilizando o **Google Colab** com uma conta\n",
        "sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/83JggUJ1mhgWviEaA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 16/10 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "Gabriel Gomes de Oliveira Costa  \n",
        "11201921471\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "Beatriz Domingos de Oliveira\n",
        "11202130893`\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: Cap 9 `\n",
        "\n",
        "`Segundo capítulo: Cap 17`\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` para identificar ERROS em 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        "Os capítulos devem ser selecionados na seguinte planilha:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**DICA:** Por favor, insira o seu nome ou da sua equipe na ordem definida na planilha. Por exemplo, se a linha correspondente ao o GRUPO 5 já foi preenchida, a próxima equipe (GRUPO 6) deverá ser informada na próxima linha da planilha.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TIPOS DE ERROS**\n",
        "---\n"
      ],
      "metadata": {
        "id": "eD_AJQhrwJQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: consulta feita no ChatGPT\n",
        ">\n",
        "\n",
        "Um `programa Python` que utilize `expressões regulares` pode ajudar a identificar vários **tipos de erros** comuns em **livros**, especialmente erros de formatação e problemas relacionados à consistência do texto. Aqui estão alguns exemplos de erros comuns que podem ser identificados usando expressões regulares:\n",
        "\n",
        "* Erros de gramática e ortografia: erros de digitação, concordância verbal e nominal, uso incorreto de pontuação e outros erros gramaticais.\n",
        "\n",
        "* Problemas de formatação: você pode usar expressões regulares para encontrar erros de formatação, como espaços em excesso, tabulações inadequadas ou alinhamentos inconsistentes.\n",
        "\n",
        "* Abreviações e acrônimos: você pode usar expressões regulares para encontrar abreviações ou acrônimos que não foram definidos ou explicados anteriormente no texto.\n",
        "\n",
        "* Citações e referências: expressões regulares podem ser úteis para localizar citações ou referências que precisam de formatação especial.\n",
        "\n",
        "* OUTROS TIPOS DE ERROS: não considerem apenas os tipos de erros citados acima.\n",
        "\n",
        "\n",
        "**IMPORTANTE:** Lembre-se de que expressões regulares podem ser poderosas, mas também complexas. Dependendo da complexidade dos erros que você deseja identificar, pode ser necessário ajustar as expressões regulares de acordo com as características específicas do seu texto. Além disso, é importante ter em mente que as expressões regulares podem não ser a melhor ferramenta para todos os tipos de erros em livros, especialmente problemas mais contextuais ou semânticos, que podem exigir abordagens de PLN mais avançadas.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gz0DTI0KYmn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A equipe que **realizar mais testes** e/ou **identificar mais erros** terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30). Os testes e possíveis erros devem ser contabizados de maneira separada.\n",
        "\n",
        ">\n",
        "\n",
        "Além disso, **por se tratar de um livro**, há um teste importante que deve ser feito. Lembre-se que o teste deve ser feito utilizando expressões regulares. A equipe que realizar esse teste, mesmo que o erro não ocorra nos capítulos selecionados, terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30).\n",
        "\n",
        "> A equipe pode considerar outros capítulos do livro para tentar identificar esse tipo de erro.\n",
        "\n",
        "**Se for a mesma equipe, o peso da avaliação será reduzido em 50% (caindo de 40 para 20)**.\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE**: a diminuição no peso da AVALIAÇÃO será aplicado para todos os membros da equipe. Esse critério será aplicado apenas para uma equipe, considerando como critério de desempate a equipe que entregar primeiro a atividade no formulário.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the constants\n",
        "URL_CHAPTER_9 = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte5/cap9/cap9.html'\n",
        "URL_CHAPTER_17 = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap17/cap17.html'"
      ],
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#define function to scrap information about the page\n",
        "def scrap_text_information(url) :\n",
        "   response = requests.get(url);\n",
        "   soup = BeautifulSoup(response.content, 'html.parser')\n",
        "   paragraphs = soup.find_all('p')                                                #correcao bia\n",
        "   scraped_text = '\\n'.join(paragraph.get_text() for paragraph in paragraphs)     #correcao bia\n",
        "\n",
        "   #scraped_text = soup.get_text()                                                #codigo origem\n",
        "   return scraped_text"
      ],
      "metadata": {
        "id": "PcTpq5ga8KaF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chapther_9 = scrap_text_information(URL_CHAPTER_9)\n",
        "text_chapther_17 = scrap_text_information(URL_CHAPTER_17)"
      ],
      "metadata": {
        "id": "Rx5ZmLIjJqJY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there is a date in the format MM/DD/YYYY or YYYY/MM/DD as the article is in PT-BR so the dates must be DD/MM/YYYY\n",
        "import re\n",
        "\n",
        "def check_usa_pattern(text) :\n",
        "    usa_pattern = r'(?:0[1-9]|1[0-2])/(?:0[1-9]|[1-2][0-9]|3[0-1])/\\d{4}'\n",
        "    matches = re.findall(usa_pattern, text)\n",
        "    is_text_valid = True\n",
        "    date_list = []\n",
        "    for match in matches:\n",
        "        if not re.search(r'^(0[1-9]|[1-2][0-9]|3[0-1])/(0[1-9]|1[0-2])/\\d{4}$', match):\n",
        "            is_text_valid = False\n",
        "            date_list.append(match)\n",
        "    return (is_text_valid, date_list)\n",
        "\n",
        "def check_iso_pattern(text) :\n",
        "    iso_pattern = r'\\d{4}/(?:0[1-9]|1[0-2])/(?:0[1-9]|[1-2][0-9]|3[0-1])'\n",
        "    matches = re.findall(iso_pattern, text)\n",
        "    return (len(matches) == 0, matches)\n",
        "\n",
        "def valid_date_format(text, text_title) :\n",
        "    check_usa = check_usa_pattern(text)\n",
        "    if check_usa[0] :\n",
        "      print(f'{text_title} : Not Found MM/DD/YYYY pattern :)')\n",
        "    else :\n",
        "       for date in check_usa[1]:\n",
        "          print(f\"{text_title} : Found MM/DD/YYYY date: {date}\")\n",
        "    check_iso = check_iso_pattern(text)\n",
        "    if check_iso[0] :\n",
        "      print(f'{text_title} : Not Found YYYY/MM/DD pattern :)')\n",
        "    else :\n",
        "       for date in check_iso[1]:\n",
        "          print(f\"{text_title} : Found YYYY/MM/DD date: {date}\")"
      ],
      "metadata": {
        "id": "lliS2f7POX-U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check whether sentences start with capital letters\n",
        "#disregards sentences beginning with lowercase letters after the abbreviated expressions \"i.e.\" \"e.g.\" and \"al.\"\n",
        "import re\n",
        "\n",
        "def check_start_capital_letters(text,text_title):\n",
        "    padrao = r'(?<!\\bi.e)(?<!\\bal)(?<!\\be.g)[.!?](?:\\s|\\n)+[a-z]'\n",
        "    matches = re.finditer(padrao, text)\n",
        "    cont = 0\n",
        "    for match in matches:\n",
        "      trecho = match.group()\n",
        "      #print(f'Sentence starting with a lowercase letter found in {text_title}: \"{trecho}\"') #comment/uncomment if you want to display or not the excerpts with identified errors\n",
        "      cont += 1\n",
        "    print(f'{cont} sentences starting with lowercase letters were found in {text_title}')\n"
      ],
      "metadata": {
        "id": "Z5P1hrHWBLV_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check punctuation error\n",
        "import re\n",
        "\n",
        "def check_punctuation_error(text,text_title):\n",
        "    padrao = r'(\\s\\s+)|(?<!\\bi.e)(?<!\\bal)(?<!\\be.g)([,?!;]+[,.;?!]+)|(?<!\\bi.e)(?<!\\bal)(?<!\\be.g)([,.;]+[,?!;]+)'\n",
        "    matches = re.finditer(padrao, text)\n",
        "    cont = 0\n",
        "    for match in matches:\n",
        "      trecho = match.group()\n",
        "      #print(f'Sentence with punctuation error in {text_title}: \"{trecho}\"')      #comment/uncomment if you want to display or not the excerpts with identified errors\n",
        "      cont += 1\n",
        "    print(f'{cont} sentences with punctuation error were found in {text_title}')\n"
      ],
      "metadata": {
        "id": "4qifpJrGL85X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check formatting abbreviations\n",
        "import re\n",
        "\n",
        "def check_formatting_abbreviations(text,text_title):\n",
        "    padrao = r'[\\s\\(\\)]((ace)|(amr)|(api)|(bart)|(bbp)|(bert)|(bio)|(bio)|(br)|(card)|(cetenf)|(copa)|(crf)|(darpa)|(delph)|(dl)|(dptoie)|(ee)|(ei)|(eia)|(en)|(er)|(ere)|(flame)|(frump)|(gpt)|(harem)|(ia)|(ie)|(in)|(jasper)|(json)|(lef)|(lia)|(lstm)|(malinche)|(mb)|(mit)|(muc)|(mv)|(mwe)|(net)|(neura)|(nilc)|(nli)|(nlu)|(nnl)|(noie)|(oe)|(oie)|(omcs)|(omw)|(onto)|(org)|(osx)|(owl)|(own)|(pefoce)|(per)|(pfn)|(php)|(pln)|(prolog)|(pt)|(pulo)|(pwn)|(relp)|(ren)|(ri)|(rnn)|(sgs)|(sql)|(srl)|(svm)|(ufjf)|(ufsc)|(uri)|(wsd))[\\s\\)\\(]'\n",
        "    matches = re.finditer(padrao, text)\n",
        "    cont = 0\n",
        "    for match in matches:\n",
        "      trecho = match.group()\n",
        "      print(trecho)      #comment/uncomment if you want to display or not the excerpts with identified errors\n",
        "      cont += 1\n",
        "    print(f'{cont} sentences with different abbreviation formatting in {text_title}')\n"
      ],
      "metadata": {
        "id": "rjdOePHdtS7c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def check_agreement_problem(text,text_title):\n",
        "    padrao = r'\\b(\\w+)(?:s(?=\\s)|(?<=\\s)s)\\b'\n",
        "    matches = re.finditer(padrao, text)\n",
        "    cont = 0\n",
        "    for match in matches:\n",
        "      trecho = match.group()\n",
        "      print(f'\"{trecho}\" possible agreement problem(s) in {text_title}')      #comment/uncomment if you want to display or not the excerpts with identified errors\n",
        "      cont += 1\n",
        "    print(f'there are {cont} possible agreement problem(s) in {text_title}')"
      ],
      "metadata": {
        "id": "aGUpx9mw3Cgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def validate_and_count_urls(text):\n",
        "    # Padrão para encontrar URLs\n",
        "    url_pattern = r'https?://[^\\s]+'\n",
        "\n",
        "    # Encontrar todas as URLs no texto\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    # Inicializar contadores\n",
        "    total_urls = len(urls)\n",
        "    valid_urls = 0\n",
        "\n",
        "    for url in urls:\n",
        "        # Verificar se a URL está no formato correto\n",
        "        if re.match(url_pattern, url):\n",
        "            valid_urls += 1\n",
        "\n",
        "    return total_urls, valid_urls\n"
      ],
      "metadata": {
        "id": "zV0D4-hTtW8R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download the stopwords if not already downloaded\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Define a list of common stop words\n",
        "stop_words = set(stopwords.words(\"portuguese\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Oc0vZ7tob9",
        "outputId": "8017a3da-9083-4ace-fc4a-670e8372a530"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_redundancy_validation(text, text_title) :\n",
        "  paragraph_pattern = r'([A-Z].*?\\n)'\n",
        "  # Use re.findall to find all paragraph-like patterns\n",
        "  paragraphs = re.findall(paragraph_pattern, text)\n",
        "  word_counts = {}\n",
        "  for i, paragraph in enumerate(paragraphs, start=1):\n",
        "    # Tokenize the paragraph into words (split by spaces and punctuation)\n",
        "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())  # Convert to lowercase for case-insensitive counting\n",
        "\n",
        "    # Count word occurrences in the paragraph\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    # Check if any word appears more than 5 times\n",
        "    for word, count in word_counts.items():\n",
        "        if count > 5 and word not in stop_words:\n",
        "            print(f\"{text_title} : In Paragraph {i}, the word '{word}' appears {count} times, which is more than 5 times, please review the text there may be redundancy\")"
      ],
      "metadata": {
        "id": "9y8rLq8otv-n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_date_format(text_chapther_9, 'chapter 9')\n",
        "valid_date_format(text_chapther_17, 'chapter 17')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liv9o7O1RXRM",
        "outputId": "c68dc9f8-8338-48ea-a626-7879b938d3cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chapter 9 : Not Found MM/DD/YYYY pattern :)\n",
            "chapter 9 : Not Found YYYY/MM/DD pattern :)\n",
            "chapter 17 : Not Found MM/DD/YYYY pattern :)\n",
            "chapter 17 : Not Found YYYY/MM/DD pattern :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_start_capital_letters(text_chapther_9, 'chapter 9')\n",
        "check_start_capital_letters(text_chapther_17, 'chapter 17')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwAqrWcvuM1h",
        "outputId": "34096031-bd92-4579-e070-fe17c9925d52"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 sentences starting with lowercase letters were found in chapter 9\n",
            "1 sentences starting with lowercase letters were found in chapter 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_punctuation_error(text_chapther_9, 'chapter 9')\n",
        "check_punctuation_error(text_chapther_17, 'chapter 17')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWPp-HonuN5F",
        "outputId": "7e22a876-ea13-497b-b4ca-247f05e54202"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 sentences with punctuation error were found in chapter 9\n",
            "24 sentences with punctuation error were found in chapter 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_formatting_abbreviations(text_chapther_9, 'chapter 9')\n",
        "check_formatting_abbreviations(text_chapther_17, 'chapter 17')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0sVdONtuINR",
        "outputId": "6d8e0881-b255-4cec-f222-98ba5b50973e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 sentences with different abbreviation formatting in chapter 9\n",
            "0 sentences with different abbreviation formatting in chapter 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "add_redundancy_validation(text_chapther_9, 'chapter 9')\n",
        "add_redundancy_validation(text_chapther_17, 'chapter 17')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUO5dcEYuUk6",
        "outputId": "05ed1367-1ed4-462a-fde0-f5fb6136cdc6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chapter 9 : In Paragraph 6, the word 'conhecimento' appears 6 times, which is more than 5 times, please review the text there may be redundancy\n",
            "chapter 9 : In Paragraph 46, the word 'português' appears 6 times, which is more than 5 times, please review the text there may be redundancy\n",
            "chapter 17 : In Paragraph 43, the word 'entidades' appears 7 times, which is more than 5 times, please review the text there may be redundancy\n",
            "chapter 17 : In Paragraph 85, the word 'claro' appears 8 times, which is more than 5 times, please review the text there may be redundancy\n",
            "chapter 17 : In Paragraph 85, the word 'glauber' appears 6 times, which is more than 5 times, please review the text there may be redundancy\n",
            "chapter 17 : In Paragraph 116, the word 'avaliação' appears 6 times, which is more than 5 times, please review the text there may be redundancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total, valid = validate_and_count_urls(text_chapther_9)\n",
        "\n",
        "print(f\"Número de URLs válidas no capitulo 9: {valid}\")\n",
        "print(f\"Número total de URLs no capitulo 9: {total}\")\n",
        "\n",
        "total, valid = validate_and_count_urls(text_chapther_17)\n",
        "\n",
        "print(f\"Número de URLs válidas no capitulo 17: {valid}\")\n",
        "print(f\"Número total de URLs no capitulo 17: {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsvLwOSCt-NK",
        "outputId": "3abfa035-3bda-4914-cff8-9f567fa36fdc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de URLs válidas no capitulo 9: 30\n",
            "Número total de URLs no capitulo 9: 30\n",
            "Número de URLs válidas no capitulo 17: 1\n",
            "Número total de URLs no capitulo 17: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLaC90hfuFeJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}